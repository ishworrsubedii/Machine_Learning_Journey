{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bed98e",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41ffc2",
   "metadata": {},
   "source": [
    "- Training error is low but test error is high\n",
    "- Low bias,high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314ac93",
   "metadata": {},
   "source": [
    "## Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d510072",
   "metadata": {},
   "source": [
    "- When a model is too simple to capture the underlying patterns in the training dataset, it is said to be underfitting.\n",
    "- High bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8c5b0a",
   "metadata": {},
   "source": [
    "#### - To solve the underfitting and overfitting problems, we are going to use regularization in this notebook.\n",
    "\n",
    "###  Cost function for regularized linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde5f15",
   "metadata": {},
   "source": [
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 $$ \n",
    "\n",
    "- Lambda=10^10       --Underfit\n",
    "- Lambda=0           --Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79e62e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1dc9326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.07917239320214275\n"
     ]
    }
   ],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        cost = cost + (f_wb_i - y[i])**2             \n",
    "    cost = cost / (2 * m)  \n",
    " \n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost\n",
    "    \n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost\n",
    "\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)#scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e4a9a",
   "metadata": {},
   "source": [
    "### Cost function for regularized logistic regression\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43ae0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.6850849138741673\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    logistic = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    return logistic\n",
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b    \n",
    "        f_wb_i = sigmoid(z_i)                                          \n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      \n",
    "             \n",
    "    cost = cost/m                                                      \n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          \n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              \n",
    "    \n",
    "    total_cost = cost + reg_cost                                       \n",
    "    return total_cost                                                  \n",
    "\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19fed8",
   "metadata": {},
   "source": [
    "### Gradient function for regularized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b41dcba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b & w: (0.6648774569425727, array([0.29653215, 0.49116796, 0.21645878]))\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "print(f'b & w: {dj_db_tmp,dj_dw_tmp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5f6aa",
   "metadata": {},
   "source": [
    "### Gradient function for regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e8081",
   "metadata": {},
   "source": [
    "- Gradient descent for both linear and logistic regression have the same equation, but the f_wb_xi is different.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})  \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e31c2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: 0.341798994972791\n",
      "Regularized w: [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            \n",
    "    dj_db = 0.0                                       \n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          \n",
    "        err_i  = f_wb_i  - y[i]                       \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      \n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   \n",
    "    dj_db = dj_db/m                                   \n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  \n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"b: {dj_db_tmp}\", )\n",
    "print(f\"Regularized w: {dj_dw_tmp.tolist()}\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
